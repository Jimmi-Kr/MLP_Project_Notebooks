{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6018a750",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:23.454235Z",
     "iopub.status.busy": "2025-07-27T16:28:23.453898Z",
     "iopub.status.idle": "2025-07-27T16:28:25.393375Z",
     "shell.execute_reply": "2025-07-27T16:28:25.392342Z"
    },
    "papermill": {
     "duration": 1.950244,
     "end_time": "2025-07-27T16:28:25.394956",
     "exception": false,
     "start_time": "2025-07-27T16:28:23.444712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv\n",
      "/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\n",
      "/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7ffc2",
   "metadata": {
    "papermill": {
     "duration": 0.006886,
     "end_time": "2025-07-27T16:28:25.408969",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.402083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engage2Value:Predicting Purchase Value\n",
    "This project aims to predict the **purchase value of users** by analyzing their **multi-session browsing behavior on a digital commerce platform.**\n",
    "\n",
    "The dataset contains a wide range of features, including:\n",
    "\n",
    "> Device Information – e.g., desktop, mobile usage patterns\n",
    "\n",
    "> Traffic Sources – e.g., referrals, ad campaigns, search channels\n",
    "\n",
    "> User Session Activity – e.g., total page views, hits, session counts\n",
    "\n",
    "## Evaluation Metric:\n",
    "Submissions are evaluated using the R² score (r2_score), which measures how well the predicted values match the actual purchase values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ff513",
   "metadata": {
    "papermill": {
     "duration": 0.006318,
     "end_time": "2025-07-27T16:28:25.422340",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.416022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Required Libaries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a44925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.436969Z",
     "iopub.status.busy": "2025-07-27T16:28:25.436202Z",
     "iopub.status.idle": "2025-07-27T16:28:25.440262Z",
     "shell.execute_reply": "2025-07-27T16:28:25.439493Z"
    },
    "papermill": {
     "duration": 0.012686,
     "end_time": "2025-07-27T16:28:25.441510",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.428824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "# from sklearn.linear_model import Ridge, Lasso\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.metrics import r2_score\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f03008",
   "metadata": {
    "papermill": {
     "duration": 0.006742,
     "end_time": "2025-07-27T16:28:25.455077",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.448335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48066003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.470422Z",
     "iopub.status.busy": "2025-07-27T16:28:25.469747Z",
     "iopub.status.idle": "2025-07-27T16:28:25.473645Z",
     "shell.execute_reply": "2025-07-27T16:28:25.472771Z"
    },
    "papermill": {
     "duration": 0.01289,
     "end_time": "2025-07-27T16:28:25.474971",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.462081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d97845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.489410Z",
     "iopub.status.busy": "2025-07-27T16:28:25.489114Z",
     "iopub.status.idle": "2025-07-27T16:28:25.493012Z",
     "shell.execute_reply": "2025-07-27T16:28:25.492283Z"
    },
    "papermill": {
     "duration": 0.012736,
     "end_time": "2025-07-27T16:28:25.494313",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.481577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1154236",
   "metadata": {
    "papermill": {
     "duration": 0.006225,
     "end_time": "2025-07-27T16:28:25.507335",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.501110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 116023 rows and 52 Features in Train Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bcde6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.521941Z",
     "iopub.status.busy": "2025-07-27T16:28:25.521581Z",
     "iopub.status.idle": "2025-07-27T16:28:25.525704Z",
     "shell.execute_reply": "2025-07-27T16:28:25.525022Z"
    },
    "papermill": {
     "duration": 0.012972,
     "end_time": "2025-07-27T16:28:25.527113",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.514141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f72c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.542316Z",
     "iopub.status.busy": "2025-07-27T16:28:25.541404Z",
     "iopub.status.idle": "2025-07-27T16:28:25.545623Z",
     "shell.execute_reply": "2025-07-27T16:28:25.544820Z"
    },
    "papermill": {
     "duration": 0.012813,
     "end_time": "2025-07-27T16:28:25.546929",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.534116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.info()\n",
    "# # train dataset \n",
    "# # info()--> give all information of all features like (how many missing value , Datatype of features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8bf02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.561456Z",
     "iopub.status.busy": "2025-07-27T16:28:25.561177Z",
     "iopub.status.idle": "2025-07-27T16:28:25.565134Z",
     "shell.execute_reply": "2025-07-27T16:28:25.564377Z"
    },
    "papermill": {
     "duration": 0.012761,
     "end_time": "2025-07-27T16:28:25.566434",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.553673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f17c1",
   "metadata": {
    "papermill": {
     "duration": 0.006356,
     "end_time": "2025-07-27T16:28:25.579921",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.573565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 29006 rows and 51 features in Test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3a9676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.595307Z",
     "iopub.status.busy": "2025-07-27T16:28:25.594975Z",
     "iopub.status.idle": "2025-07-27T16:28:25.598978Z",
     "shell.execute_reply": "2025-07-27T16:28:25.598162Z"
    },
    "papermill": {
     "duration": 0.01326,
     "end_time": "2025-07-27T16:28:25.600436",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.587176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a1ada3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.615330Z",
     "iopub.status.busy": "2025-07-27T16:28:25.614998Z",
     "iopub.status.idle": "2025-07-27T16:28:25.618946Z",
     "shell.execute_reply": "2025-07-27T16:28:25.618167Z"
    },
    "papermill": {
     "duration": 0.013006,
     "end_time": "2025-07-27T16:28:25.620328",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.607322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed660b",
   "metadata": {
    "papermill": {
     "duration": 0.006318,
     "end_time": "2025-07-27T16:28:25.633464",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.627146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## EDA (Exploratory Data Analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a58dcea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.647949Z",
     "iopub.status.busy": "2025-07-27T16:28:25.647639Z",
     "iopub.status.idle": "2025-07-27T16:28:25.651401Z",
     "shell.execute_reply": "2025-07-27T16:28:25.650582Z"
    },
    "papermill": {
     "duration": 0.012746,
     "end_time": "2025-07-27T16:28:25.652839",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.640093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check for missing values in train dataset\n",
    "# missing = train_df.isnull().sum().sort_values(ascending=False)\n",
    "# print(\"Missing values:\\n\", missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99c69b",
   "metadata": {
    "papermill": {
     "duration": 0.006527,
     "end_time": "2025-07-27T16:28:25.666443",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.659916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The columns listed below contain missing values sorted in descending order of missing percentage/count.\n",
    "This helps us easily identify which columns have missing values in the training dataset.\n",
    "During preprocessing, this insight was used to handle missing data systematically and reduce information leakage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e9402e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.681058Z",
     "iopub.status.busy": "2025-07-27T16:28:25.680757Z",
     "iopub.status.idle": "2025-07-27T16:28:25.684652Z",
     "shell.execute_reply": "2025-07-27T16:28:25.683847Z"
    },
    "papermill": {
     "duration": 0.012833,
     "end_time": "2025-07-27T16:28:25.686009",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.673176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# missing_pert=((train_df.isnull().sum())/len(train_df))*100\n",
    "# missing_pert=missing_pert[missing_pert>0].sort_values(ascending=False)\n",
    "# print(missing_pert)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.bar(missing_pert.index,missing_pert.values,color='blue')\n",
    "# plt.title('Percentage of Missing Value per features in Train Data')\n",
    "# plt.xlabel('Features')\n",
    "# plt.xticks(rotation='vertical')\n",
    "# plt.ylabel('Missing_value_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbdb3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.701082Z",
     "iopub.status.busy": "2025-07-27T16:28:25.700440Z",
     "iopub.status.idle": "2025-07-27T16:28:25.704209Z",
     "shell.execute_reply": "2025-07-27T16:28:25.703459Z"
    },
    "papermill": {
     "duration": 0.012516,
     "end_time": "2025-07-27T16:28:25.705401",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.692885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# missing = test_df.isnull().sum().sort_values(ascending=False)\n",
    "# print(\"Missing values:\\n\", missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e9be3",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2025-07-27T16:28:25.719004",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.712300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The columns listed below contain missing values sorted in descending order of missing percentage/count.\n",
    "This helps us easily identify which columns have missing values in the test dataset.\n",
    "During preprocessing, this insight was used to handle missing data systematically and reduce information leakage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9379c982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.734983Z",
     "iopub.status.busy": "2025-07-27T16:28:25.734388Z",
     "iopub.status.idle": "2025-07-27T16:28:25.738428Z",
     "shell.execute_reply": "2025-07-27T16:28:25.737700Z"
    },
    "papermill": {
     "duration": 0.013168,
     "end_time": "2025-07-27T16:28:25.739783",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.726615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# missing_pert=((test_df.isnull().sum())/len(test_df))*100\n",
    "# missing_pert=missing_pert[missing_pert>0].sort_values(ascending=False)\n",
    "# print(missing_pert)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.bar(missing_pert.index,missing_pert.values,color='blue')\n",
    "# plt.title('Percentage of Missing Value per features in test Data')\n",
    "# plt.xlabel('Features')\n",
    "# plt.xticks(rotation='vertical')\n",
    "# plt.ylabel('Missing_value_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276cb8c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.754510Z",
     "iopub.status.busy": "2025-07-27T16:28:25.754237Z",
     "iopub.status.idle": "2025-07-27T16:28:25.758015Z",
     "shell.execute_reply": "2025-07-27T16:28:25.757177Z"
    },
    "papermill": {
     "duration": 0.01288,
     "end_time": "2025-07-27T16:28:25.759572",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.746692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cat_features = X_train.select_dtypes(include='object').columns\n",
    "# num_features = X_train.select_dtypes(include=np.number).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591ad79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.775141Z",
     "iopub.status.busy": "2025-07-27T16:28:25.774448Z",
     "iopub.status.idle": "2025-07-27T16:28:25.778273Z",
     "shell.execute_reply": "2025-07-27T16:28:25.777601Z"
    },
    "papermill": {
     "duration": 0.012761,
     "end_time": "2025-07-27T16:28:25.779521",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.766760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', 'passthrough', num_features),\n",
    "#     ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "122dbed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.794944Z",
     "iopub.status.busy": "2025-07-27T16:28:25.794257Z",
     "iopub.status.idle": "2025-07-27T16:28:25.798338Z",
     "shell.execute_reply": "2025-07-27T16:28:25.797477Z"
    },
    "papermill": {
     "duration": 0.013083,
     "end_time": "2025-07-27T16:28:25.799705",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.786622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', LinearRegression())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218400ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.814824Z",
     "iopub.status.busy": "2025-07-27T16:28:25.814501Z",
     "iopub.status.idle": "2025-07-27T16:28:25.818673Z",
     "shell.execute_reply": "2025-07-27T16:28:25.817610Z"
    },
    "papermill": {
     "duration": 0.013066,
     "end_time": "2025-07-27T16:28:25.819948",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.806882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred_log = pipeline.predict(X_test)\n",
    "# y_pred = np.expm1(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a3b5da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.835632Z",
     "iopub.status.busy": "2025-07-27T16:28:25.835313Z",
     "iopub.status.idle": "2025-07-27T16:28:25.839097Z",
     "shell.execute_reply": "2025-07-27T16:28:25.838231Z"
    },
    "papermill": {
     "duration": 0.013043,
     "end_time": "2025-07-27T16:28:25.840469",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.827426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame({\"id\": range(0,X_test.shape[0]), \"purchaseValue\": y_pred}) \n",
    "# submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc903f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.856843Z",
     "iopub.status.busy": "2025-07-27T16:28:25.856519Z",
     "iopub.status.idle": "2025-07-27T16:28:25.862118Z",
     "shell.execute_reply": "2025-07-27T16:28:25.861239Z"
    },
    "papermill": {
     "duration": 0.015266,
     "end_time": "2025-07-27T16:28:25.863496",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.848230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # ---------------------\n",
    "# # Load Data\n",
    "# # ---------------------\n",
    "# df_train = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# df_test = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "# # ---------------------\n",
    "# # Preprocessing Function\n",
    "# # ---------------------\n",
    "# def preprocess(df):\n",
    "#     df = df.copy()\n",
    "\n",
    "#     # Drop columns with >50% missing\n",
    "#     missing_percent = df.isnull().mean() * 100\n",
    "#     df = df.drop(columns=missing_percent[missing_percent > 50].index)\n",
    "\n",
    "#     # Drop columns with only 1 unique value\n",
    "#     df = df.drop(columns=[col for col in df.columns if df[col].nunique() == 1])\n",
    "\n",
    "#     # Drop identifiers\n",
    "#     id_cols = ['userId', 'sessionId', 'sessionStart']\n",
    "#     df = df.drop(columns=[col for col in id_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "#     # Convert date\n",
    "#     if 'date' in df.columns:\n",
    "#         df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "#         df['weekday'] = df['date'].dt.weekday\n",
    "#         df['month'] = df['date'].dt.month\n",
    "#         df = df.drop(columns='date')\n",
    "\n",
    "#     # Frequency encoding for high-cardinality features\n",
    "#     for col in ['browser', 'city', 'region']:\n",
    "#         if col in df.columns:\n",
    "#             freq = df[col].value_counts(normalize=True)\n",
    "#             df[f'{col}_freq'] = df[col].map(freq)\n",
    "#             df.drop(columns=col, inplace=True)\n",
    "\n",
    "#     # Handle missing values\n",
    "#     num_cols = df.select_dtypes(include=np.number).columns\n",
    "#     cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "#     for col in num_cols:\n",
    "#         if df[col].isnull().any():\n",
    "#             df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "#     for col in cat_cols:\n",
    "#         if df[col].isnull().any():\n",
    "#             df[col] = df[col].fillna('missing')\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ---------------------\n",
    "# # Preprocess Data\n",
    "# # ---------------------\n",
    "# df_train_processed = preprocess(df_train)\n",
    "# df_test_processed = preprocess(df_test)\n",
    "\n",
    "# # ---------------------\n",
    "# # Features & Target\n",
    "# # ---------------------\n",
    "# X_train = df_train_processed.drop(columns='purchaseValue')\n",
    "# y_train = np.log1p(df_train_processed['purchaseValue'])  # log-transform target\n",
    "# X_test = df_test_processed.copy()\n",
    "\n",
    "# cat_features = X_train.select_dtypes(include='object').columns\n",
    "# num_features = X_train.select_dtypes(include=np.number).columns\n",
    "\n",
    "# # ---------------------\n",
    "# # Pipeline & Model\n",
    "# # ---------------------\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', 'passthrough', num_features),\n",
    "#     ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "# ])\n",
    "\n",
    "# model = XGBRegressor(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=6,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', model)\n",
    "# ])\n",
    "\n",
    "# # ---------------------\n",
    "# # Train Model\n",
    "# # ---------------------\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # ---------------------\n",
    "# # Predict on Test\n",
    "# # ---------------------\n",
    "# y_test_pred_log = pipeline.predict(X_test)\n",
    "# y_test_pred = np.expm1(y_test_pred_log)  # back-transform\n",
    "\n",
    "# # ---------------------\n",
    "# # Generate Submission File Without Using sample_submission\n",
    "# # ---------------------\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': np.arange(len(y_test_pred)),\n",
    "#     'purchaseValue': y_test_pred\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a4442",
   "metadata": {
    "papermill": {
     "duration": 0.006725,
     "end_time": "2025-07-27T16:28:25.877411",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.870686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Final code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f662fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.892652Z",
     "iopub.status.busy": "2025-07-27T16:28:25.892331Z",
     "iopub.status.idle": "2025-07-27T16:28:25.896473Z",
     "shell.execute_reply": "2025-07-27T16:28:25.895821Z"
    },
    "papermill": {
     "duration": 0.013379,
     "end_time": "2025-07-27T16:28:25.897757",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.884378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "# df_train = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# df_test = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "\n",
    "# X_train = df_train.select_dtypes(include=np.number).copy()\n",
    "# X_test = df_test.select_dtypes(include=np.number).copy()\n",
    "\n",
    "\n",
    "# y_train = X_train.pop('purchaseValue')\n",
    "\n",
    "# X_train = X_train.fillna(0)\n",
    "# X_test = X_test.fillna(0)\n",
    "\n",
    "# missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "# for col in missing_cols:\n",
    "#     X_test[col] = 0  \n",
    "\n",
    "# X_test = X_test[X_train.columns]\n",
    "\n",
    "# model = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': np.arange(len(y_test_pred)),\n",
    "#     'purchaseValue': y_test_pred\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb34bcc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.913639Z",
     "iopub.status.busy": "2025-07-27T16:28:25.913368Z",
     "iopub.status.idle": "2025-07-27T16:28:25.918869Z",
     "shell.execute_reply": "2025-07-27T16:28:25.918154Z"
    },
    "papermill": {
     "duration": 0.015238,
     "end_time": "2025-07-27T16:28:25.920204",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.904966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Predicting Purchase Value with ExtraTreesRegressor\n",
    "# # This notebook processes the dataset, creates an engagement score feature,\n",
    "# # trains an ExtraTreesRegressor model, and generates predictions for submission.\n",
    "\n",
    "# # ===============================\n",
    "# # 📦 1. Import Required Libraries\n",
    "# # ===============================\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import os\n",
    "\n",
    "# # ===============================\n",
    "# # 📂 2. Load and Prepare Datasets\n",
    "# # ===============================\n",
    "# # Create copies of the original datasets to avoid modifying them\n",
    "# train_df_org = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# test_df_org = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "# train_df = train_df_org.copy()\n",
    "# test_df = test_df_org.copy()\n",
    "\n",
    "# # ===============================\n",
    "# # 🛠 3. Data Preprocessing\n",
    "# # ===============================\n",
    "# # Combine train and test data for consistent preprocessing\n",
    "# combined_df = pd.concat([train_df.drop('purchaseValue', axis=1), test_df], ignore_index=True)\n",
    "\n",
    "# # Identify categorical columns\n",
    "# categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# # Handle missing values in specific columns using the recommended method\n",
    "# combined_df['new_visits'] = combined_df['new_visits'].fillna(0.0)\n",
    "# combined_df['totals.bounces'] = combined_df['totals.bounces'].fillna(0.0)\n",
    "\n",
    "# # Fill remaining missing values with 0\n",
    "# combined_df.fillna(0.0, inplace=True)\n",
    "\n",
    "# # Separate the combined data back into training and testing sets\n",
    "# X_train = combined_df.iloc[:len(train_df)].copy()\n",
    "# test_df = combined_df.iloc[len(train_df):].copy()\n",
    "# y_train = train_df['purchaseValue']\n",
    "\n",
    "# # Drop categorical columns after splitting\n",
    "# X_train.drop(columns=categorical_cols, errors='ignore', inplace=True)\n",
    "# test_df.drop(columns=categorical_cols, errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # 🔧 4. Feature Engineering\n",
    "# # ===============================\n",
    "# # Convert date to datetime\n",
    "# X_train['date_dt'] = pd.to_datetime(X_train['date'].astype(str).str.zfill(8), format='%Y%m%d')\n",
    "# test_df['date_dt'] = pd.to_datetime(test_df['date'].astype(str).str.zfill(8), format='%Y%m%d')\n",
    "\n",
    "# # Extract day of week and month\n",
    "# X_train['day_of_week'] = X_train['date_dt'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "# X_train['month'] = X_train['date_dt'].dt.month\n",
    "# test_df['day_of_week'] = test_df['date_dt'].dt.dayofweek\n",
    "# test_df['month'] = test_df['date_dt'].dt.month\n",
    "\n",
    "# # Create cyclical features for day of year\n",
    "# X_train['day_of_year'] = X_train['date_dt'].dt.dayofyear\n",
    "# X_train['date_day_sin'] = np.sin(2 * np.pi * X_train['day_of_year'] / 365)\n",
    "# X_train['date_day_cos'] = np.cos(2 * np.pi * X_train['day_of_year'] / 365)\n",
    "# test_df['day_of_year'] = test_df['date_dt'].dt.dayofyear\n",
    "# test_df['date_day_sin'] = np.sin(2 * np.pi * test_df['day_of_year'] / 365)\n",
    "# test_df['date_day_cos'] = np.cos(2 * np.pi * test_df['day_of_year'] / 365)\n",
    "\n",
    "# # Drop temporary and original date columns\n",
    "# X_train = X_train.drop(columns=['date', 'date_dt', 'day_of_year'], errors='ignore')\n",
    "# test_df = test_df.drop(columns=['date', 'date_dt', 'day_of_year'], errors='ignore')\n",
    "\n",
    "# # Create engagement score feature based on total hits, page views, and session number\n",
    "# X_train['engagement_score'] = (\n",
    "#     X_train['totalHits'] + X_train['pageViews'] + X_train['sessionNumber']\n",
    "# )\n",
    "# test_df['engagement_score'] = (\n",
    "#     test_df['totalHits'] + test_df['pageViews'] + test_df['sessionNumber']\n",
    "# )\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # 🤖 5. Model Training\n",
    "# # ===============================\n",
    "# # Initialize the ExtraTreesRegressor model\n",
    "# model = ExtraTreesRegressor(\n",
    "#     n_estimators=100,        # Number of trees in the forest\n",
    "#     random_state=42,         # Ensure reproducibility\n",
    "#     n_jobs=-1                # Utilize all available CPU cores\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # ===============================\n",
    "# # 🔍 6. Predictions\n",
    "# # ===============================\n",
    "# # Generate predictions on test data\n",
    "# y_pred = model.predict(test_df)\n",
    "\n",
    "# # ===============================\n",
    "# # 💾 7. Create Submission File\n",
    "# # ===============================\n",
    "# # Prepare submission DataFrame\n",
    "# submission_df = pd.DataFrame({\n",
    "#     \"id\": range(0, test_df.shape[0]),\n",
    "#     \"purchaseValue\": y_pred\n",
    "# })\n",
    "\n",
    "# # Save submission to CSV\n",
    "# submission_df.to_csv('submission_engg_final.csv', index=False)\n",
    "\n",
    "# # Print confirmation\n",
    "# print(\"Submission file 'submission.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82aabf2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.935594Z",
     "iopub.status.busy": "2025-07-27T16:28:25.935273Z",
     "iopub.status.idle": "2025-07-27T16:28:25.939302Z",
     "shell.execute_reply": "2025-07-27T16:28:25.938581Z"
    },
    "papermill": {
     "duration": 0.01335,
     "end_time": "2025-07-27T16:28:25.940627",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.927277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(X_train.columns.tolist())\n",
    "# print(X_train.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c66fc61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.958224Z",
     "iopub.status.busy": "2025-07-27T16:28:25.957606Z",
     "iopub.status.idle": "2025-07-27T16:28:25.964219Z",
     "shell.execute_reply": "2025-07-27T16:28:25.963255Z"
    },
    "papermill": {
     "duration": 0.017932,
     "end_time": "2025-07-27T16:28:25.965821",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.947889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ===============================\n",
    "# # 📦 1. Import Required Libraries\n",
    "# # ===============================\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # ===============================\n",
    "# # 📂 2. Load and Prepare Datasets\n",
    "# # ===============================\n",
    "# train_df_org = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# test_df_org = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "# train_df = train_df_org.copy()\n",
    "# test_df = test_df_org.copy()\n",
    "\n",
    "# # ===============================\n",
    "# # 🛠 3. Data Preprocessing\n",
    "# # ===============================\n",
    "# combined_df = pd.concat([train_df.drop('purchaseValue', axis=1), test_df], ignore_index=True)\n",
    "\n",
    "# categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# combined_df['new_visits'] = combined_df['new_visits'].fillna(0.0)\n",
    "# combined_df['totals.bounces'] = combined_df['totals.bounces'].fillna(0.0)\n",
    "# combined_df.fillna(0.0, inplace=True)\n",
    "\n",
    "# X_train = combined_df.iloc[:len(train_df)].copy()\n",
    "# X_test = combined_df.iloc[len(train_df):].copy()\n",
    "# y_train = train_df['purchaseValue']\n",
    "\n",
    "# X_train.drop(columns=categorical_cols, errors='ignore', inplace=True)\n",
    "# X_test.drop(columns=categorical_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# # ===============================\n",
    "# # 🔧 4. Feature Engineering\n",
    "# # ===============================\n",
    "# # Date features\n",
    "# X_train['date_dt'] = pd.to_datetime(X_train['date'].astype(str).str.zfill(8), format='%Y%m%d')\n",
    "# X_test['date_dt'] = pd.to_datetime(X_test['date'].astype(str).str.zfill(8), format='%Y%m%d')\n",
    "\n",
    "# X_train['day_of_week'] = X_train['date_dt'].dt.dayofweek\n",
    "# X_train['month'] = X_train['date_dt'].dt.month\n",
    "# X_train['day_of_year'] = X_train['date_dt'].dt.dayofyear\n",
    "# X_train['date_day_sin'] = np.sin(2 * np.pi * X_train['day_of_year'] / 365)\n",
    "# X_train['date_day_cos'] = np.cos(2 * np.pi * X_train['day_of_year'] / 365)\n",
    "\n",
    "# X_test['day_of_week'] = X_test['date_dt'].dt.dayofweek\n",
    "# X_test['month'] = X_test['date_dt'].dt.month\n",
    "# X_test['day_of_year'] = X_test['date_dt'].dt.dayofyear\n",
    "# X_test['date_day_sin'] = np.sin(2 * np.pi * X_test['day_of_year'] / 365)\n",
    "# X_test['date_day_cos'] = np.cos(2 * np.pi * X_test['day_of_year'] / 365)\n",
    "\n",
    "# # Engagement score\n",
    "# X_train['engagement_score'] = X_train['totalHits'] + X_train['pageViews'] + X_train['sessionNumber']\n",
    "# X_test['engagement_score'] = X_test['totalHits'] + X_test['pageViews'] + X_test['sessionNumber']\n",
    "\n",
    "# # Interaction features\n",
    "# X_train['hits_per_session'] = X_train['totalHits'] / (X_train['sessionNumber'] + 1)\n",
    "# X_train['views_per_hit'] = X_train['pageViews'] / (X_train['totalHits'] + 1)\n",
    "\n",
    "# X_test['hits_per_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)\n",
    "# X_test['views_per_hit'] = X_test['pageViews'] / (X_test['totalHits'] + 1)\n",
    "\n",
    "# # Drop ID and date columns\n",
    "# drop_cols = ['userId', 'sessionId', 'sessionStart', 'date', 'date_dt', 'day_of_year']\n",
    "# X_train.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "# X_test.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# # ===============================\n",
    "# # 🔁 5. Log-Transform Target\n",
    "# # ===============================\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "# # ===============================\n",
    "# # ⚙️ 6. Hyperparameter Tuning (Optional)\n",
    "# # ===============================\n",
    "# model = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Use this block if you want to tune\n",
    "# param_dist = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_features': ['sqrt', 'log2', None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# search = RandomizedSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=10,\n",
    "#     scoring='neg_root_mean_squared_error',\n",
    "#     cv=3,\n",
    "#     verbose=1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# search.fit(X_train, y_train_log)\n",
    "# best_model = search.best_estimator_\n",
    "\n",
    "# # ===============================\n",
    "# # 📉 7. Cross-Validation Score\n",
    "# # ===============================\n",
    "# cv_score = cross_val_score(best_model, X_train, y_train_log, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# print(f\"Average CV RMSE (log-scale): {-np.mean(cv_score):.4f}\")\n",
    "\n",
    "# # ===============================\n",
    "# # 🤖 8. Final Training and Prediction\n",
    "# # ===============================\n",
    "# best_model.fit(X_train, y_train_log)\n",
    "# y_pred_log = best_model.predict(X_test)\n",
    "# y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# # ===============================\n",
    "# # 💾 9. Submission File\n",
    "# # ===============================\n",
    "# submission_df = pd.DataFrame({\n",
    "#     \"id\": range(0, X_test.shape[0]),\n",
    "#     \"purchaseValue\": y_pred\n",
    "# })\n",
    "# submission_df.to_csv('submission_engg_optimized.csv', index=False)\n",
    "\n",
    "# print(\"✅ Submission file 'submission_engg_optimized.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88bc1d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:25.981750Z",
     "iopub.status.busy": "2025-07-27T16:28:25.981430Z",
     "iopub.status.idle": "2025-07-27T16:28:25.985395Z",
     "shell.execute_reply": "2025-07-27T16:28:25.984650Z"
    },
    "papermill": {
     "duration": 0.013386,
     "end_time": "2025-07-27T16:28:25.986768",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.973382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sns.histplot(y_train, bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f9d89a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.002804Z",
     "iopub.status.busy": "2025-07-27T16:28:26.001971Z",
     "iopub.status.idle": "2025-07-27T16:28:26.005444Z",
     "shell.execute_reply": "2025-07-27T16:28:26.004839Z"
    },
    "papermill": {
     "duration": 0.012667,
     "end_time": "2025-07-27T16:28:26.006693",
     "exception": false,
     "start_time": "2025-07-27T16:28:25.994026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.info()\n",
    "# train_df.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3709fdb",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.082127Z",
     "iopub.status.busy": "2025-07-27T16:28:26.081496Z",
     "iopub.status.idle": "2025-07-27T16:28:26.086872Z",
     "shell.execute_reply": "2025-07-27T16:28:26.086044Z"
    },
    "papermill": {
     "duration": 0.015336,
     "end_time": "2025-07-27T16:28:26.088504",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.073168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # --------------------------\n",
    "# # Load Data\n",
    "# # --------------------------\n",
    "# df_train = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# df_test = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "# # --------------------------\n",
    "# # Keep Numeric Columns Only\n",
    "# # --------------------------\n",
    "# X_train = df_train.select_dtypes(include=np.number).copy()\n",
    "# X_test = df_test.select_dtypes(include=np.number).copy()\n",
    "\n",
    "# y_train = X_train.pop('purchaseValue')\n",
    "\n",
    "# # --------------------------\n",
    "# # Fill Missing Values\n",
    "# # --------------------------\n",
    "# X_train.fillna(0, inplace=True)\n",
    "# X_test.fillna(0, inplace=True)\n",
    "\n",
    "# # --------------------------\n",
    "# # Align Train/Test Columns\n",
    "# # --------------------------\n",
    "# missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "# for col in missing_cols:\n",
    "#     X_test[col] = 0\n",
    "# X_test = X_test[X_train.columns]\n",
    "\n",
    "# # --------------------------\n",
    "# # Feature Selection with ExtraTrees\n",
    "# # --------------------------\n",
    "# base_model = ExtraTreesRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# base_model.fit(X_train, y_train)\n",
    "\n",
    "# # Keep top N important features (tune as needed)\n",
    "# N = 30\n",
    "# importances = base_model.feature_importances_\n",
    "# important_indices = np.argsort(importances)[::-1][:N]\n",
    "# important_features = X_train.columns[important_indices]\n",
    "\n",
    "# X_train_imp = X_train[important_features]\n",
    "# X_test_imp = X_test[important_features]\n",
    "\n",
    "# # --------------------------\n",
    "# # LightGBM Model\n",
    "# # --------------------------\n",
    "# lgb_model = lgb.LGBMRegressor(\n",
    "#     n_estimators=1000,\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=7,\n",
    "#     num_leaves=31,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# lgb_model.fit(X_train_imp, y_train)\n",
    "\n",
    "# # --------------------------\n",
    "# # CatBoost Model (Optional)\n",
    "# # --------------------------\n",
    "# # cb_model = CatBoostRegressor(\n",
    "# #     iterations=1000,\n",
    "# #     learning_rate=0.05,\n",
    "# #     depth=7,\n",
    "# #     loss_function='RMSE',\n",
    "# #     verbose=100,\n",
    "# #     random_seed=42\n",
    "# # )\n",
    "# # cb_model.fit(X_train_imp, y_train)\n",
    "\n",
    "# # --------------------------\n",
    "# # Predict and Export\n",
    "# # --------------------------\n",
    "# y_test_pred = lgb_model.predict(X_test_imp)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': np.arange(len(y_test_pred)),\n",
    "#     'purchaseValue': y_test_pred\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2fd3b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.105081Z",
     "iopub.status.busy": "2025-07-27T16:28:26.104769Z",
     "iopub.status.idle": "2025-07-27T16:28:26.108588Z",
     "shell.execute_reply": "2025-07-27T16:28:26.107838Z"
    },
    "papermill": {
     "duration": 0.013576,
     "end_time": "2025-07-27T16:28:26.110007",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.096431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.histplot(df_train['purchaseValue'], bins=100)\n",
    "# plt.yscale('log')\n",
    "# plt.title('Distribution of purchaseValue')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Percentage of zeros:\", (df_train['purchaseValue'] == 0).mean() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b7ba4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.126374Z",
     "iopub.status.busy": "2025-07-27T16:28:26.125726Z",
     "iopub.status.idle": "2025-07-27T16:28:26.129501Z",
     "shell.execute_reply": "2025-07-27T16:28:26.128759Z"
    },
    "papermill": {
     "duration": 0.013525,
     "end_time": "2025-07-27T16:28:26.131048",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.117523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cat_cols = df_train.select_dtypes(include='object').columns.tolist()\n",
    "# for col in cat_cols:\n",
    "#     print(f\"{col}: {df_train[col].nunique()} unique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f58d281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.147316Z",
     "iopub.status.busy": "2025-07-27T16:28:26.146998Z",
     "iopub.status.idle": "2025-07-27T16:28:26.151372Z",
     "shell.execute_reply": "2025-07-27T16:28:26.150441Z"
    },
    "papermill": {
     "duration": 0.014235,
     "end_time": "2025-07-27T16:28:26.152905",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.138670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load your data\n",
    "# df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "\n",
    "# # Only numeric columns\n",
    "# num_cols = df.select_dtypes(include=np.number)\n",
    "\n",
    "# # Drop ID columns if any\n",
    "# num_cols = num_cols.drop(columns=['userId', 'sessionId'], errors='ignore')\n",
    "\n",
    "# # Remove rows with missing target\n",
    "# num_cols = num_cols[num_cols['purchaseValue'].notna()]\n",
    "\n",
    "# # Compute correlations\n",
    "# correlation = num_cols.corr()['purchaseValue'].sort_values(ascending=False)\n",
    "\n",
    "# # Display top correlated features\n",
    "# print(\"Top correlated features with purchaseValue:\")\n",
    "# print(correlation.head(15))\n",
    "\n",
    "# # Optional: plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(x=correlation.head(15).values, y=correlation.head(15).index)\n",
    "# plt.title(\"Top Correlated Numerical Features with purchaseValue\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c9fed7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.169518Z",
     "iopub.status.busy": "2025-07-27T16:28:26.169238Z",
     "iopub.status.idle": "2025-07-27T16:28:26.173518Z",
     "shell.execute_reply": "2025-07-27T16:28:26.172859Z"
    },
    "papermill": {
     "duration": 0.014054,
     "end_time": "2025-07-27T16:28:26.174804",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.160750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "\n",
    "# # List of categorical columns (you can expand this list)\n",
    "# categorical_cols = [\n",
    "#     'trafficSource.medium', 'geoNetwork.continent',\n",
    "#     'geoNetwork.subContinent', 'deviceType', 'userChannel',\n",
    "#     'trafficSource.campaign', 'geoNetwork.region', 'geoNetwork.city'\n",
    "# ]\n",
    "\n",
    "# # Filter only rows with valid target\n",
    "# df = df[df['purchaseValue'].notna()]\n",
    "\n",
    "# cat_summary = {}\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     if df[col].nunique() < 100:  # limit for plotting\n",
    "#         # Mean target per category\n",
    "#         grouped = df.groupby(col)['purchaseValue'].mean().sort_values(ascending=False)\n",
    "#         cat_summary[col] = grouped\n",
    "\n",
    "#         # Plot it\n",
    "#         grouped.plot(kind='bar', figsize=(10, 4), title=f\"Mean purchaseValue by {col}\")\n",
    "#         plt.ylabel('Mean purchaseValue')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f84265a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:28:26.191394Z",
     "iopub.status.busy": "2025-07-27T16:28:26.191129Z",
     "iopub.status.idle": "2025-07-27T16:29:13.554997Z",
     "shell.execute_reply": "2025-07-27T16:29:13.554003Z"
    },
    "papermill": {
     "duration": 47.373982,
     "end_time": "2025-07-27T16:29:13.556435",
     "exception": false,
     "start_time": "2025-07-27T16:28:26.182453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated R² scores: [0.64264319 0.41551984 0.70597318 0.48760957 0.41592127]\n",
      "Mean CV R²: 0.5335\n",
      "✅ Submission file 'submission_engg_final.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 📦 1. Import Required Libraries\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# 📂 2. Load and Prepare Datasets\n",
    "# ===============================\n",
    "train_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "# Save target separately\n",
    "y_train = train_df['purchaseValue']\n",
    "\n",
    "# Drop target from training features\n",
    "train_df = train_df.drop(columns=['purchaseValue'])\n",
    "\n",
    "# Combine train and test data for uniform preprocessing\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# ===============================\n",
    "# 🧹 3. Data Cleaning & Preprocessing\n",
    "# ===============================\n",
    "# Drop constant/low-variance categorical columns\n",
    "categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "combined_df.drop(columns=categorical_cols, errors='ignore', inplace=True)\n",
    "\n",
    "# Fill important missing values\n",
    "combined_df['new_visits'] = combined_df['new_visits'].fillna(0.0)\n",
    "combined_df['totals.bounces'] = combined_df['totals.bounces'].fillna(0.0)\n",
    "combined_df.fillna(0.0, inplace=True)\n",
    "\n",
    "# ===============================\n",
    "# 📅 4. Feature Engineering\n",
    "# ===============================\n",
    "# Convert and extract date-based features\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'].astype(str).str.zfill(8), format='%Y%m%d')\n",
    "combined_df['day_of_week'] = combined_df['date'].dt.dayofweek\n",
    "combined_df['month'] = combined_df['date'].dt.month\n",
    "combined_df['day_of_year'] = combined_df['date'].dt.dayofyear\n",
    "combined_df['date_day_sin'] = np.sin(2 * np.pi * combined_df['day_of_year'] / 365)\n",
    "combined_df['date_day_cos'] = np.cos(2 * np.pi * combined_df['day_of_year'] / 365)\n",
    "combined_df.drop(columns=['date', 'day_of_year'], inplace=True)\n",
    "\n",
    "# Engagement score: domain-driven feature\n",
    "combined_df['engagement_score'] = (\n",
    "    combined_df['totalHits'] + combined_df['pageViews'] + combined_df['sessionNumber']\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 🔄 5. Split Back to Train/Test\n",
    "# ===============================\n",
    "X_train = combined_df.iloc[:len(y_train)].copy()\n",
    "X_test = combined_df.iloc[len(y_train):].copy()\n",
    "\n",
    "# ===============================\n",
    "# 🤖 6. Model Training: ExtraTrees\n",
    "# ===============================\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optional: Cross-validation (comment out if not needed)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-validated R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Fit model on full training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # ===============================\n",
    "# # 📊 7. Feature Importance Plot\n",
    "# # ===============================\n",
    "# importances = model.feature_importances_\n",
    "# features = X_train.columns\n",
    "# feat_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "# feat_df = feat_df.sort_values(by='Importance', ascending=False).head(15)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x='Importance', y='Feature', data=feat_df, palette='viridis')\n",
    "# plt.title('Top 15 Feature Importances (ExtraTrees)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 🔍 8. Generate Predictions\n",
    "# ===============================\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ===============================\n",
    "# 💾 9. Create Submission File\n",
    "# ===============================\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": range(0, X_test.shape[0]),\n",
    "    \"purchaseValue\": y_pred\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Submission file 'submission_engg_final.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af1b008d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.573063Z",
     "iopub.status.busy": "2025-07-27T16:29:13.572758Z",
     "iopub.status.idle": "2025-07-27T16:29:13.576701Z",
     "shell.execute_reply": "2025-07-27T16:29:13.575990Z"
    },
    "papermill": {
     "duration": 0.013806,
     "end_time": "2025-07-27T16:29:13.578020",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.564214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cat_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
    "# print(\"Remaining categorical columns:\", cat_cols)\n",
    "\n",
    "# # Optional: show top unique values in key ones\n",
    "# for col in ['channelGrouping', 'device.deviceCategory', 'geoNetwork.country']:\n",
    "#     if col in train_df.columns:\n",
    "#         print(f\"\\n{col} top values:\")\n",
    "#         print(train_df[col].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "232c0b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.595117Z",
     "iopub.status.busy": "2025-07-27T16:29:13.594782Z",
     "iopub.status.idle": "2025-07-27T16:29:13.600045Z",
     "shell.execute_reply": "2025-07-27T16:29:13.599259Z"
    },
    "papermill": {
     "duration": 0.015487,
     "end_time": "2025-07-27T16:29:13.601382",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.585895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # ---------------- Load Data ----------------\n",
    "# train_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# test_df = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "# # ---------------- Target ----------------\n",
    "# y_train = train_df['purchaseValue'].copy()\n",
    "\n",
    "# # Drop ID column if present\n",
    "# id_col = 'id' if 'id' in test_df.columns else None\n",
    "\n",
    "# # ---------------- Column Separation ----------------\n",
    "# num_cols = train_df.select_dtypes(include=np.number).drop(columns=['purchaseValue']).columns.tolist()\n",
    "# cat_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# # ---------------- Feature Engineering ----------------\n",
    "\n",
    "# def frequency_encode(df_train, df_test, column):\n",
    "#     freq = df_train[column].value_counts()\n",
    "#     df_train[column + '_freq'] = df_train[column].map(freq)\n",
    "#     df_test[column + '_freq'] = df_test[column].map(freq)\n",
    "#     return df_train, df_test\n",
    "\n",
    "# def target_mean_encode(df_train, df_test, column, target):\n",
    "#     mean_target = df_train.groupby(column)[target].mean()\n",
    "#     df_train[column + '_targetEnc'] = df_train[column].map(mean_target)\n",
    "#     df_test[column + '_targetEnc'] = df_test[column].map(mean_target)\n",
    "#     return df_train, df_test\n",
    "\n",
    "# # Encode high-cardinality categoricals\n",
    "# high_card_cols = [col for col in cat_cols if train_df[col].nunique() > 50]\n",
    "\n",
    "# for col in high_card_cols:\n",
    "#     train_df, test_df = target_mean_encode(train_df, test_df, col, 'purchaseValue')\n",
    "\n",
    "# # Encode remaining categoricals with frequency\n",
    "# low_card_cols = [col for col in cat_cols if col not in high_card_cols]\n",
    "# for col in low_card_cols:\n",
    "#     train_df, test_df = frequency_encode(train_df, test_df, col)\n",
    "\n",
    "# # ---------------- Feature Selection ----------------\n",
    "# # Final set of features\n",
    "# final_cols = [col for col in train_df.columns if col not in ['purchaseValue'] + cat_cols]\n",
    "\n",
    "# X_train = train_df[final_cols].copy()\n",
    "# X_test = test_df[final_cols].copy()\n",
    "\n",
    "# # Fill remaining NaNs with 0\n",
    "# X_train = X_train.fillna(0)\n",
    "# X_test = X_test.fillna(0)\n",
    "\n",
    "# # ---------------- LightGBM ----------------\n",
    "# lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'r2',\n",
    "#     'verbosity': -1,\n",
    "#     'random_state': 42,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'num_leaves': 64,\n",
    "#     'n_estimators': 1000\n",
    "# }\n",
    "\n",
    "# model = lgb.LGBMRegressor(**params)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # ---------------- Predict and Save ----------------\n",
    "# preds = model.predict(X_test)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': np.arange(len(preds)),\n",
    "#     'purchaseValue': preds\n",
    "# })\n",
    "# submission.to_csv('submission_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "726944a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.618501Z",
     "iopub.status.busy": "2025-07-27T16:29:13.618214Z",
     "iopub.status.idle": "2025-07-27T16:29:13.621929Z",
     "shell.execute_reply": "2025-07-27T16:29:13.621214Z"
    },
    "papermill": {
     "duration": 0.013692,
     "end_time": "2025-07-27T16:29:13.623173",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.609481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(df_train['purchaseValue'].describe())\n",
    "# print(df_train.select_dtypes(include='object').nunique().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3e1c178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.640026Z",
     "iopub.status.busy": "2025-07-27T16:29:13.639393Z",
     "iopub.status.idle": "2025-07-27T16:29:13.643109Z",
     "shell.execute_reply": "2025-07-27T16:29:13.642329Z"
    },
    "papermill": {
     "duration": 0.013449,
     "end_time": "2025-07-27T16:29:13.644425",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.630976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fe372df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.661451Z",
     "iopub.status.busy": "2025-07-27T16:29:13.660910Z",
     "iopub.status.idle": "2025-07-27T16:29:13.664828Z",
     "shell.execute_reply": "2025-07-27T16:29:13.664024Z"
    },
    "papermill": {
     "duration": 0.013958,
     "end_time": "2025-07-27T16:29:13.666300",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.652342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Simple correlation heatmap for top 20 numeric features\n",
    "# corr = df_train.corr(numeric_only=True)\n",
    "# top_corr = corr['purchaseValue'].abs().sort_values(ascending=False).head(20)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=top_corr.values, y=top_corr.index)\n",
    "# plt.title('Top Numeric Correlations with purchaseValue')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0b27a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T16:29:13.683303Z",
     "iopub.status.busy": "2025-07-27T16:29:13.682716Z",
     "iopub.status.idle": "2025-07-27T16:29:13.687033Z",
     "shell.execute_reply": "2025-07-27T16:29:13.686377Z"
    },
    "papermill": {
     "duration": 0.014037,
     "end_time": "2025-07-27T16:29:13.688281",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.674244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# # Load data\n",
    "# df_train = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')\n",
    "# df_test = pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')\n",
    "\n",
    "# # Separate target\n",
    "# y_train = df_train['purchaseValue']\n",
    "\n",
    "# # Selected numeric features based on correlation\n",
    "# top_num_features = ['totalHits', 'pageViews', 'sessionNumber', 'device.isMobile']\n",
    "\n",
    "# # Selected categorical features (low to moderate cardinality)\n",
    "# cat_features = [\n",
    "#     'geoNetwork.continent',\n",
    "#     'trafficSource.medium',\n",
    "#     'userChannel',\n",
    "#     'geoNetwork.subContinent',\n",
    "#     'trafficSource.campaign'\n",
    "# ]\n",
    "\n",
    "# # Combine for modeling\n",
    "# X_train = df_train[top_num_features + cat_features].copy()\n",
    "# X_test = df_test[top_num_features + cat_features].copy()\n",
    "\n",
    "# # Fill missing numerics\n",
    "# X_train[top_num_features] = X_train[top_num_features].fillna(0)\n",
    "# X_test[top_num_features] = X_test[top_num_features].fillna(0)\n",
    "\n",
    "# # Frequency Encoding for categoricals\n",
    "# for col in cat_features:\n",
    "#     freq = X_train[col].value_counts(normalize=True)\n",
    "#     X_train[col] = X_train[col].map(freq).fillna(0)\n",
    "#     X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "\n",
    "# # Final fill\n",
    "# X_test = X_test[X_train.columns]\n",
    "\n",
    "# # Train model\n",
    "# model = ExtraTreesRegressor(n_estimators=150, max_depth=18, random_state=42, n_jobs=-1)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict\n",
    "# y_test_pred = model.predict(X_test)\n",
    "\n",
    "# # Output submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': np.arange(len(y_test_pred)),\n",
    "#     'purchaseValue': y_test_pred\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ff4a4",
   "metadata": {
    "papermill": {
     "duration": 0.007458,
     "end_time": "2025-07-27T16:29:13.703629",
     "exception": false,
     "start_time": "2025-07-27T16:29:13.696171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11895149,
     "sourceId": 99546,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 56.166771,
   "end_time": "2025-07-27T16:29:14.431333",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-27T16:28:18.264562",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
